{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods \n",
    "\n",
    "## Yue Dong & Ali Emami\n",
    "In this assignment, Ali and I experimented the policy gradient methods on Mountain car problem and easy21 (a simplified version of blackjack), as suggested in chapter 13 of the textbook. We first implemented the REINFORCE which is a policy gradient algorithm based on the complete return as in Monte Carlo algorithm. We compared the result of REINFORCE with or without the baseline. We then implemented the actor-critic methods with 1 or n step return. Lastly, we used actor-critic algorithm with eligibility trace and $\\lambda$-return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In policy gradient learning, instead of using $Q(s,a)$ to choose the best action in a certain state, we select an action based on the *learned parameterized policy* $\\pi(a | s,\\mathbb{\\theta})$. In more details, the action a is choosen with the probability $\\pi(a | s,\\mathbb{\\theta})$ given the state $s$ at time $t$ with the weight vector $\\theta$.\n",
    "\n",
    "Although selecting actions is not based on consulting a value function in policy gradient algorithm, sometimes, we still need a parameterized value function. For example, in REINFORCE, we need $V$ as the baseline and in actor-critic with 1-step or n-steps or $\\lambda$-returns, we need $V$ to form the return.  We therefore could use function approximation to parameterize the value function with respect to the weights $w$. Thus, $\\hat{V}=V(s,w)$.\n",
    "\n",
    "### (a) policy approximation \n",
    "Since the action space is discrete in both mountain car and easy21, we from the parameterization of the policy with the preferences $h(s,a, \\theta)$ which is similar to the case in multi-arm bandit. Then **the policy** is defined as an exponential softmax distribution (h bigger for $a$, $a$ is more likely to be chosen):\n",
    "$$\\pi(a|s,\\theta)=\\frac{exp(h(s,a,\\theta))}{\\sum_b exp(h(s,b,\\theta))}, b\\in A_s$$\n",
    "\n",
    "Here we choose a linear function approxiamation in features to represent the **preferences**: \n",
    "$$h(s,a,\\theta) = \\theta^{T} \\phi(s,a)$$ where **the features** $\\phi(s,a) \\in \\mathbb{R}^n$ is constructed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) In easy21,  we define  $\\phi(s,a)$ as a binary feature vector with $3*6*2 = 36$ features. \n",
    "Each binary feature\n",
    " has a value of 1 if (s,a) lies within the cuboid of state-space corresponding to\n",
    " that feature, and the action corresponding to that feature. The cuboids have\n",
    " the following overlapping intervals:\n",
    "   - dealer(s) = [1: 4]; [4: 7]; [7: 10]\n",
    "   - player(s) = [1: 6]; [4: 9]; [7: 12]; [10: 15]; [13: 18]; [16: 21]\n",
    "   - a = 1 (hit); a = 0 (stick)\n",
    "\n",
    "#### (ii) In mountain car,  we define  $\\phi(s,a)$  as a binary feature vector obtained from tile coding with  (4∗9∗9)*3  features. \n",
    "we divide the 2D space into an 8x8 grid and then we offset it 3 times with 1/4 of a tile size to form 4 tilings.  We add one extra row and one extra column so that every point is covered by each tiling. A simple tile coding with offset is demonstraded as the following graph from Sutton's textbook. \n",
    "<img src=\"tile_coding.png\" style=\"max-width:100%; width: 100%; max-width: none\">\n",
    "\n",
    "### Advantages of using policy parameterization over action-value parameterization:\n",
    "-  the approximate policy can approach determinism rather than $\\epsilon$ greedy algorithm.\n",
    "-  useful when the policy is a simpler function to approximate than action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of policy learning here is to modify $\\theta$ such that the performance measure $\\eta(\\theta)$ is maximized.\n",
    "$$\\theta_{t+1}=\\theta_{t}+\\alpha \\nabla \\eta  (\\theta_t)$$\n",
    "\n",
    "If we use $\\eta(\\theta)=v_{\\pi_\\theta}(s_0)$, then the gradient of $\\eta(\\theta)$ could be represented by \n",
    "<img src=\"policy_gradient_thm.png\" style=\"max-width:50%; width: 50%; max-width: none\">\n",
    "Which is called **the policy gradient theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REINFORCE: Monte Carlo Policy Gradient\n",
    "Using the policy gradient theorem, we could deduce that \n",
    "$$\\nabla \\eta(\\theta) = E_{\\pi}\\big[\\gamma^tG_t\\frac{\\nabla_{\\theta}\\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)}\\big]$$\n",
    "We sample $G_t$ on each time step and define the update as: \n",
    "<img src=\"reinforce.png\" style=\"max-width:50%; width: 50%; max-width: none\">\n",
    "Since $G_t$ is the complete return from time $t$, REINFORCE is a Monte Carlo method.\n",
    "<img src=\"reinforce_psedo.png\" style=\"max-width:80%; width: 80%; max-width: none\">\n",
    "where $$\\nabla_{\\theta}log\\pi(A_t|S_t,\\theta) =  \\frac{\\nabla_{\\theta}\\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)}$$ \n",
    "and with linear action preferences, \n",
    "<img src=\"reinforce_update.png\" style=\"max-width:50%; width: 50%; max-width: none\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/yuedong/Downloads/comp767_assignment_5/\")\n",
    "#%%\n",
    "\n",
    "from easy21game import Easy21\n",
    "from easy21feature import *\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "#%%\n",
    "env = Easy21()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block implements the policy approximation\n",
    "\n",
    "def preference_cal(theta, state, action):\n",
    "    h_s_a = np.dot(theta, phi(state,action))\n",
    "    return h_s_a\n",
    "\n",
    "def policy(theta, state, action):\n",
    "    # there are 36 features\n",
    "    h_s_a = preference_cal(theta, state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sarsa_Agent:\n",
    "    def __init__(self, environment, mlambda, gamma=1, step_size=0.01):\n",
    "\n",
    "        self.env = environment\n",
    "        self.mlambda = mlambda\n",
    "        self.gamma = gamma\n",
    "        self.step_size = step_size\n",
    "        #initialize everything\n",
    "#        self.Q = np.zeros((self.env.dealer_space,\n",
    "#                           self.env.player_space, \n",
    "#                           self.env.nA))\n",
    "         \n",
    "        self.V = np.zeros((self.env.dealer_space,\n",
    "                           self.env.player_space))\n",
    "        \n",
    "        self.W = np.zeros(36)\n",
    "        self.E = np.zeros(36)\n",
    "        \n",
    "        self.iterations = 0\n",
    "        \n",
    "        \n",
    "    # Q is simply the dot product of phi and w\n",
    "    def cal_Q(self,s,a):\n",
    "        return np.dot(phi(s,a),self.W)\n",
    "\n",
    "\n",
    "          # get optimal action based on ε-greedy exploration strategy  \n",
    "    def epsilon_greedy_action(self, state, epsilon=0.05):\n",
    "        # action = 0 stick\n",
    "        # action = 1 hit\n",
    "        hit = 1\n",
    "        stick = 0\n",
    "        # epsilon greedy policy\n",
    "        if np.random.random() < epsilon:\n",
    "            r_action = hit if np.random.random()<0.5 else stick\n",
    "            return r_action\n",
    "        else:\n",
    "            action = np.argmax([self.cal_Q(state,0),self.cal_Q(state,1)])\n",
    "            return action\n",
    "        \n",
    "    \n",
    "    def train(self, iterations):        \n",
    "        # Loop episodes\n",
    "        for episode in range(iterations):\n",
    "            self.E = np.zeros(36)\n",
    "\n",
    "            # get initial state for current episode\n",
    "            s = self.env._reset()\n",
    "            a = self.epsilon_greedy_action(s)\n",
    "            a_next = a\n",
    "            term = False\n",
    "            #r = 0\n",
    "            \n",
    "            # Execute until game ends\n",
    "            while not term:\n",
    "                \n",
    "                # execute action\n",
    "                s_next, r, term = self.env._step(a)[0:3]\n",
    "                q = self.cal_Q(s,a)\n",
    "                                \n",
    "                if not term:\n",
    "                    # choose next action with epsilon greedy policy\n",
    "                    a_next = self.epsilon_greedy_action(s_next)\n",
    "                    q_next = self.cal_Q(s_next,a_next)\n",
    "                    delta = r + self.gamma * q_next - q\n",
    "                else:\n",
    "                    delta = r - q\n",
    "                \n",
    "                \n",
    "                self.E =  self.gamma * self.mlambda * self.E +phi(s,a)\n",
    "                self.W = self.W + self.step_size * delta * self.E\n",
    "\n",
    "                # reassign s and a\n",
    "                s = s_next\n",
    "                a = a_next\n",
    "\n",
    "\n",
    "        self.iterations += iterations\n",
    "\n",
    "        # Derive value function\n",
    "        for d in range(self.env.dealer_space):\n",
    "            for p in range(self.env.player_space):\n",
    "                self.V[d,p] = max([self.cal_Q((p+1,d+1),0),self.cal_Q((p+1,d+1),1)])\n",
    "                \n",
    "    def plot_frame(self, ax):\n",
    "        def get_stat_val(x, y):\n",
    "            return self.V[x, y]\n",
    "\n",
    "        X = np.arange(0, self.env.dealer_space, 1)\n",
    "        Y = np.arange(0, self.env.player_space, 1)\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = get_stat_val(X, Y)\n",
    "        \n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                               cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "        return surf\n",
    "#%%\n",
    "\n",
    "agent = Sarsa_Agent(env, 0.5)\n",
    "#%%\n",
    "for i in range (1):\n",
    "    agent.train(10000)\n",
    "\n",
    "#print(\"learned values:\",agent.V)\n",
    "#agent.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Mountain-Car task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) mountain car environment\n",
    "We first implemented the environment of the mountain car based on the example 8.2 (first version of sutton's book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) feature encoding with tile coding \n",
    "We encode the feature space of the mountaion car as 4 tilings of 8x8 grids with a shift of 1/4 of a tile size. We add one extra row and one extra column so that every point is covered by each tiling. Since we are doing control, the features $\\phi(s,a)$ are defined based on states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# this code is inspired from https://github.com/ctevans/.../Tilecoder.py\n",
    "\n",
    "# mountain car has two variables: position(x-axis) and velocity\n",
    "# -1.2 <= position <= 0.5\n",
    "# -0.07 <= velocity <= 0.07\n",
    "\n",
    "# devide the 2D space into an 8 by 8 grid\n",
    "# then shift this grid with 1/4 of a tile width to obtain 4 tilings (partitions) \n",
    "numTiles = 8 \n",
    "numTilings = 4\n",
    "\n",
    "positionTileMove = ((0.5 + 1.2) / numTiles) / numTilings\n",
    "velocityTileMove = ((0.07 + 0.07) / numTiles) /numTilings\n",
    "\n",
    "# in order to make sure all points are covered after shifting \n",
    "# add one extra row and one extra column for shifting\n",
    "# numver of total features  = 9x9x4\n",
    "numFeatures = numTilings * (numTiles+1) * (numTiles+1)\n",
    "\n",
    "\n",
    "# x = position, y = velocity\n",
    "# note move a tiling by (a, b), then find the index of a point\n",
    "# is the same as moving the points by (-a, -b)\n",
    "# shift direction in this code is to the left-bottom corner\n",
    "def tilecode(x,y,tileIndices):\n",
    "    \n",
    "    # find the distance of x to the leftmost position\n",
    "    x = x + 1.2\n",
    "    # find the distance of y to smallest velocity\n",
    "    y = y + 0.07\n",
    "\n",
    "    for i in range (numTilings):\n",
    "        \n",
    "        # in tiling i, we move a points by \n",
    "        # (-i*positionTileMove,i*velocityTileMove) for feature encoding\n",
    "        xMove = i * (-positionTileMove)\n",
    "        yMove = i * (-velocityTileMove)\n",
    "\t\n",
    "        xTileIdx = int(numTiles * (x - xMove)/1.7)\n",
    "        yTileIdx = int(numTiles * (y - yMove)/0.14)\n",
    "\n",
    "        tileIndices[i] = i * 81 + ( yTileIdx * 9 + xTileIdx)\n",
    "    \n",
    "    \n",
    "def tileCoderIndices(x,y):\n",
    "    tileIndices = [-1]*numTilings\n",
    "    tilecode(x,y,tileIndices)\n",
    "    #print('Tile indices for input (%s,%s) are : '%(x,y), tileIndices)\n",
    "    return tileIndices\n",
    "                                                \n",
    "#printTileCoderIndices(0.5,0.07)\n",
    "#[-1, -1, -1, -1]\n",
    "#Tile indices for input (0.5,0.07) are :  [80, 161, 242, 323]\n",
    "\n",
    "# INPUTS\n",
    "#   s: state =(position,velocity) \n",
    "#   a: action, integer: throttle reverse (-1), zero throttle (0), throttle forwards (1)\n",
    "# returns a binary vector of length (4*9*9)*3 representing the features\n",
    "def phi(s, a):\n",
    "    tmp = np.zeros(shape=(4*9*9,3)) #zeros array of dim 3*6*2\n",
    "    #putting one where a feature is on\n",
    "    for i in tileCoderIndices(s[0],s[1]):\n",
    "            tmp[i,a] = 1 \n",
    "    return(tmp.flatten()) #returning 'vectorized' (1-dim) array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) SARSA($\\lambda$) with LA\n",
    "We then modified SARSA($\\lambda$) with LA for the mountain car problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
